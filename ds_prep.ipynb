{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyP0gpyx10skZ2FbJ/WGFGlw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/purushottampanchal/temp/blob/main/ds_prep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "FGt-erDljJWH"
      },
      "outputs": [],
      "source": [
        "#@title gd connect\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title variables\n",
        "dataset_name = \"training-new\" #@param {type:\"string\"}\n",
        "output_directory = \"traineroutput\" #@param {type:\"string\"}\n",
        "upload_dir = \"Uploads\" #@param {type:\"string\"}\n",
        "upload_dir = \"/content/drive/MyDrive/\" + upload_dir\n",
        "#!mkdir $upload_dir"
      ],
      "metadata": {
        "cellView": "form",
        "id": "lUuwGjKOjgRC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install yt-dlp"
      ],
      "metadata": {
        "id": "Ief-dvPTjlRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!yt-dlp -x --audio-format wav -o $upload_dir/training_audio3.wav https://www.youtube.com/watch?v=9eMp9OAyjQs"
      ],
      "metadata": {
        "id": "Fn0LUthNk6Vj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!yt-dlp -x --audio-format wav -o $upload_dir/training_audio2.wav https://www.youtube.com/watch?v=CsceDX5sJJ8"
      ],
      "metadata": {
        "id": "A2cxpEx2kzb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!yt-dlp -x --audio-format wav -o $upload_dir/training_audio1.wav https://www.youtube.com/watch?v=-Rt_cIz67fk"
      ],
      "metadata": {
        "id": "zxN0pbVtkr-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyloudnorm\n",
        "!git clone https://github.com/xiph/rnnoise.git\n",
        "!sudo apt-get install curl autoconf automake libtool python-dev pkg-config sox ffmpeg\n",
        "%cd /content/rnnoise\n",
        "!sh autogen.sh\n",
        "!sh configure\n",
        "!make clean\n",
        "!make"
      ],
      "metadata": {
        "id": "4PtKZc0LlRA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content\n",
        "!sudo apt install sox\n",
        "!git clone https://github.com/openai/whisper.git\n",
        "!pip install git+https://github.com/openai/whisper.git \n",
        "\n"
      ],
      "metadata": {
        "id": "aoytBNP9ldaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title split and delete\n",
        "%cd $upload_dir\n",
        "!rm -rf $upload_dir/out\n",
        "!mkdir $upload_dir/out\n",
        "!find . -name '*.mp3' -exec bash -c 'for f; do ffmpeg -i \"$f\" -acodec pcm_s16le -ar 22050 -ac 1 out/\"${f%.mp3}\".wav ; done' _ {} +\n",
        "!find . -name '*.wav' -exec bash -c 'for f; do ffmpeg -i \"$f\" -acodec pcm_s16le -ar 22050 -ac 1 out/\"${f%.wav}\".wav ; done' _ {} +\n",
        "\n",
        "\n",
        "!ls -al $upload_dir/out\n",
        "!mkdir $upload_dir/out/splits\n",
        "%cd $upload_dir/out\n",
        "!for FILE in *.wav; do sox \"$FILE\" splits/\"$FILE\" --show-progress silence 1 0.5 0.1% 1 0.5 0.1% : newfile : restart ; done\n",
        "#alt split method: force splits of 8 seconds, however this will split words. Comment the above with # and remove the # below to change\n",
        "#!for FILE in *.wav; do sox \"$FILE\" splits/\"$FILE\" --show-progress trim 0 8 : restart ; done\n",
        "%cd $upload_dir/out/splits\n",
        "!find . -name \"*.wav\" -type f -size -15k -delete"
      ],
      "metadata": {
        "cellView": "form",
        "id": "zYgMUsHmlo1R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title converting wavs\n",
        "orig_wavs= upload_dir + \"/out/splits\"\n",
        "from pathlib import Path\n",
        "import os\n",
        "import subprocess\n",
        "import soundfile as sf\n",
        "import pyloudnorm as pyln\n",
        "import sys\n",
        "import glob\n",
        "rnn = \"/content/rnnoise/examples/rnnoise_demo\"\n",
        "paths = glob.glob(os.path.join(orig_wavs, '*.wav'))\n",
        "for filepath in paths:\n",
        "  base = os.path.basename(filepath)\n",
        "  tp_s = \"/content/drive/MyDrive/\" + dataset_name + \"/\" + \"converted\" + \"/\"\n",
        "  tf_s = \"/content/drive/MyDrive/\" + dataset_name + \"/\" + \"converted\" + \"/\" + base\n",
        "  target_path = Path(tp_s)\n",
        "  target_file = Path(tf_s)\n",
        "  print(\"From: \" + str(filepath))\n",
        "  print(\"To: \" + str(target_file))\n",
        "\t\n",
        "# Stereo to Mono; upsample to 48000Hz\n",
        "# added -G to fix gain, -v 0.95\n",
        "  subprocess.run([\"sox\", \"-G\", \"-v\", \"0.95\", filepath, \"48k.wav\", \"remix\", \"-\", \"rate\", \"48000\"])\n",
        "  subprocess.run([\"sox\", \"48k.wav\", \"-c\", \"1\", \"-r\", \"48000\", \"-b\", \"16\", \"-e\", \"signed-integer\", \"-t\", \"raw\", \"temp.raw\"]) # convert wav to raw\n",
        "  subprocess.run([\"/content/rnnoise/examples/rnnoise_demo\", \"temp.raw\", \"rnn.raw\"]) # apply rnnoise\n",
        "  subprocess.run([\"sox\", \"-G\", \"-v\", \"0.95\", \"-r\", \"48k\", \"-b\", \"16\", \"-e\", \"signed-integer\", \"rnn.raw\", \"-t\", \"wav\", \"rnn.wav\"]) # convert raw back to wav\n",
        "\n",
        "  subprocess.run([\"mkdir\", \"-p\", str(target_path)])\n",
        "  subprocess.run([\"sox\", \"rnn.wav\", str(target_file), \"remix\", \"-\", \"highpass\", \"100\", \"lowpass\", \"7000\", \"rate\", \"22050\"]) # apply high/low pass filter and change sr to 22050Hz\n",
        "  data, rate = sf.read(target_file)\n",
        "\n",
        "# peak normalize audio to -1 dB\n",
        "  peak_normalized_audio = pyln.normalize.peak(data, -1.0)\n",
        "\n",
        "# measure the loudness first\n",
        "  meter = pyln.Meter(rate) # create BS.1770 meter\n",
        "  loudness = meter.integrated_loudness(data)\n",
        "\n",
        "# loudness normalize audio to -25 dB LUFS\n",
        "  loudness_normalized_audio = pyln.normalize.loudness(data, loudness, -25.0)\n",
        "  sf.write(target_file, data=loudness_normalized_audio, samplerate=22050)\n",
        "  print(\"\")\n",
        "%cd /content/drive/MyDrive/\"$dataset_name\"/converted/\n",
        "!find . -name \"*.wav\" -type f -size -15k -delete\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "mdOGORcTlxU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Copy wav files to dataset wav directory\n",
        "!mkdir /content/drive/MyDrive/$dataset_name/wavs\n",
        "!cp /content/drive/MyDrive/$dataset_name/converted/*.wav /content/drive/MyDrive/$dataset_name/wavs\n",
        "!ls -al /content/drive/MyDrive/$dataset_name/wavs"
      ],
      "metadata": {
        "id": "ZmOrTv1R0Pfs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "import os, os.path\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "wavs = '/content/drive/MyDrive/'+dataset_name+'/wavs'\n",
        "\n",
        "model = whisper.load_model(\"medium.en\")\n",
        "paths = glob.glob(os.path.join(wavs, '*.wav'))\n",
        "print(len(paths))"
      ],
      "metadata": {
        "id": "0TLlJKN31ail"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Run Whisper on generated audio clips, generate transcript named metadata.csv in LJSpeech format in the dataset directory.\n",
        "import whisper\n",
        "import os, os.path\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "wavs = '/content/drive/MyDrive/'+dataset_name+'/wavs'\n",
        "\n",
        "# model = whisper.load_model(\"medium.en\")\n",
        "paths = glob.glob(os.path.join(wavs, '*.wav'))\n",
        "print(len(paths))\n",
        "\n",
        "all_filenames = []\n",
        "transcript_text = []\n",
        "with open('/content/drive/MyDrive/'+dataset_name+'/metadata.csv', 'w', encoding='utf-8') as outfile:\n",
        "\tfor filepath in paths:\n",
        "\t\tbase = os.path.basename(filepath)\n",
        "\t\tall_filenames.append(base)\n",
        "\tfor filepath in paths:\n",
        "\t\tresult = model.transcribe(filepath)\n",
        "\t\toutput = result[\"text\"].lstrip()\n",
        "\t\toutput = output.replace(\"\\n\",\"\")\n",
        "\t\tthefile = str(os.path.basename(filepath).lstrip(\".\")).rsplit(\".\")[0]\n",
        "\t\toutfile.write(thefile + '|' + output + '|' + output + '\\n')\n",
        "\t\tprint(thefile + '|' + output + '|' + output + '\\n')\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "ri4xCNui0hHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Display dataset\n",
        "!cat /content/drive/MyDrive/$dataset_name/metadata.csv"
      ],
      "metadata": {
        "id": "M9qG2sWqaLjY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title generating trainer_vits.py\n",
        "code = \"\"\"import os\n",
        "\n",
        "from trainer import Trainer, TrainerArgs\n",
        "\n",
        "from TTS.tts.configs.shared_configs import BaseDatasetConfig\n",
        "from TTS.tts.configs.vits_config import VitsConfig\n",
        "from TTS.tts.datasets import load_tts_samples\n",
        "from TTS.tts.models.vits import Vits, VitsAudioConfig\n",
        "from TTS.tts.utils.text.tokenizer import TTSTokenizer\n",
        "from TTS.utils.audio import AudioProcessor\n",
        "\n",
        "#output_path = os.path.dirname(os.path.abspath(__file__))\n",
        "##########################################\n",
        "#Change this to your dataset directory\n",
        "##########################################\n",
        "output_path = \"/content/drive/MyDrive/\"\"\"\n",
        "code = code + dataset_name + \"/\" + output_directory + \"/\" + \"\\\"\"\n",
        "\n",
        "code=code + \"\"\"\n",
        "dataset_config = BaseDatasetConfig(\n",
        "##########################################\n",
        "#Change this to your dataset directory\n",
        "##########################################\n",
        "    formatter=\"ljspeech\", meta_file_train=\"metadata.csv\", path=os.path.join(output_path, \"/content/drive/MyDrive/\"\"\"\n",
        "code = code + dataset_name\n",
        "code=code + \"\"\"\")\n",
        "\n",
        ")\n",
        "audio_config = VitsAudioConfig(\n",
        "    sample_rate=22050, win_length=1024, hop_length=256, num_mels=80, mel_fmin=0, mel_fmax=None\n",
        ")\n",
        "\n",
        "config = VitsConfig(\n",
        "    audio=audio_config,\n",
        "    run_name=\"vits_ljspeech\",\n",
        "    batch_size=8,\n",
        "    eval_batch_size=8,\n",
        "    batch_group_size=4,\n",
        "#    num_loader_workers=8,\n",
        "    num_loader_workers=4,\n",
        "    num_eval_loader_workers=4,\n",
        "    run_eval=True,\n",
        "    test_delay_epochs=-1,\n",
        "    epochs=100000,\n",
        "    save_step=1000,\n",
        "\t  save_checkpoints=True,\n",
        "\t  save_n_checkpoints=3,\n",
        "\t  save_best_after=1000,\n",
        "    #text_cleaner=\"english_cleaners\",\n",
        "    text_cleaner=\"multilingual_cleaners\",\n",
        "    use_phonemes=True,\n",
        "    phoneme_language=\"en-us\",\n",
        "    phoneme_cache_path=os.path.join(output_path, \"phoneme_cache\"),\n",
        "    compute_input_seq_cache=True,\n",
        "    print_step=25,\n",
        "    print_eval=True,\n",
        "    mixed_precision=True,\n",
        "    output_path=output_path,\n",
        "    datasets=[dataset_config],\n",
        "    cudnn_benchmark=False,\n",
        ")\n",
        "\n",
        "# INITIALIZE THE AUDIO PROCESSOR\n",
        "# Audio processor is used for feature extraction and audio I/O.\n",
        "# It mainly serves to the dataloader and the training loggers.\n",
        "ap = AudioProcessor.init_from_config(config)\n",
        "\n",
        "# INITIALIZE THE TOKENIZER\n",
        "# Tokenizer is used to convert text to sequences of token IDs.\n",
        "# config is updated with the default characters if not defined in the config.\n",
        "tokenizer, config = TTSTokenizer.init_from_config(config)\n",
        "\n",
        "# LOAD DATA SAMPLES\n",
        "# Each sample is a list of ```[text, audio_file_path, speaker_name]```\n",
        "# You can define your custom sample loader returning the list of samples.\n",
        "# Or define your custom formatter and pass it to the `load_tts_samples`.\n",
        "# Check `TTS.tts.datasets.load_tts_samples` for more details.\n",
        "train_samples, eval_samples = load_tts_samples(\n",
        "    dataset_config,\n",
        "    eval_split=True,\n",
        "    eval_split_max_size=config.eval_split_max_size,\n",
        "    eval_split_size=config.eval_split_size,\n",
        ")\n",
        "\n",
        "# init model\n",
        "model = Vits(config, ap, tokenizer, speaker_manager=None)\n",
        "\n",
        "# init the trainer and begin\n",
        "trainer = Trainer(\n",
        "    TrainerArgs(),\n",
        "    config,\n",
        "    output_path,\n",
        "    model=model,\n",
        "    train_samples=train_samples,\n",
        "    eval_samples=eval_samples,\n",
        ")\n",
        "trainer.fit()\n",
        "\"\"\"\n",
        "#print(code)\n",
        "myFile = open(\"/content/drive/MyDrive/\" + dataset_name + \"/train_vits.py\", 'w')\n",
        "\n",
        "myFile.write(code)\n",
        "myFile.close()\n",
        "%ls -al /content/drive/MyDrive/$dataset_name/train_vits.py"
      ],
      "metadata": {
        "id": "2E6BCZpraWVt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}